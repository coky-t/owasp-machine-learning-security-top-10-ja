---
layout: col-sidebar
type: documentation
altfooter: true
level: 4
auto-migrated: 0
pitch:
document: OWASP Machine Learning Security Top Ten 2023
year: 2023
order: 6
title: ML06:2023 Corrupted Packages (破損パッケージ)
lang: ja
author:
contributors:
tags: OWASP Machine Learning Security Top Ten 2023, Top Ten, ML06:2023, mltop10, mlsectop10
exploitability: 5
prevalence:
detectability: 2
technical: 4
redirect_from:
---

| 脅威エージェント/攻撃手法 | セキュリティ上の弱点 | 影響 |
|:-------------------------:|:--------------------:|:----:|
| 悪用難易度: 5 (この弱点を悪用するために必要な労力は中程度です)<br>ML アプリケーション依存: 5 <br>ML オペレーション依存: 3 | 検出難易度: 2<br>(この攻撃の検出はそれほど困難ではありません) | 技術的影響: 4 <br>(中程度の技術スキルが必要です)<br> |
| 悪意のある攻撃者<br>機械学習プロジェクトで使用するオープンソースパッケージを改変します | 信頼できないサードパーティコードに依存しています | 機械学習プロジェクトの危殆化と組織への損害の可能性 |

本チャートは下記のシナリオに基づくサンプルに過ぎず、実際のリスク評価は各機械学習システムの具体的な状況によって異なることに注意することが重要です。



**説明:**

破損パッケージ攻撃は攻撃者がシステムで使用される機械学習ライブラリやモデルを改変したり置換することで発生します。


**攻撃シナリオの例:**

シナリオ 1: 組織内の機械学習プロジェクトへの攻撃

悪意のある攻撃者は大規模な組織で開発されている機械学習プロジェクトを侵害しようとしています。
攻撃者はプロジェクトがいくつかのパッケージやライブラリに依存していることを知っており、プロジェクトを侵害する方法を見つけたいと考えています。



攻撃者は NumPy や Scikit-learn などのプロジェクトが依存しているパッケージの一つのコードを改変することで攻撃を実行しました。
攻撃者はこの改変版のパッケージを PyPI などのパブリックリポジトリにアップロードし、他の人がダウンロードして使用できるようにします。
標的となった組織がこのパッケージをダウンロードしてインストールすると、攻撃者の悪意のあるコードもインストールされ、そのプロジェクトを侵害するために使用される可能性があります。





この種の攻撃は被害者が使用しているパッケージが侵害されていることに気付かない可能性があり、長期間見つからない可能性があるため、特に危険です。
攻撃者の悪意あるコードを使用して、機密情報を盗んだり、結果を改変したり、機械学習モデルの誤作動を引き起こす可能性があります。




**防止方法:**

パッケージ署名を検証する: パッケージをインストールする前に、パッケージのデジタル署名を検証し、改竄されていないことを確認します。



安全なパッケージリポジトリを使用する: Anaconda などの厳格なセキュリティ対策を実施してパッケージの審査プロセスがある安全なパッケージリポジトリを使用します。



パッケージを最新に保つ: すべてのパッケージを定期的に更新して、脆弱性にパッチが適用されることを確保します。


仮想環境を使用する: 仮想環境を使用して、パッケージやライブラリをシステムの他の部分から分離します。
これにより悪意のあるパッケージの検知と削除が容易になります。


コードレビューを実施する: プロジェクトで使用するすべてのパッケージとライブラリについて定期的にコードレビューを実施して、悪意のあるコードを検知します。


パッケージ検証ツールを使用する: PEP 476 や Secure Package Install などのツールを使用して、インストール前にパッケージの真正性と完全性を検証します。



開発者を教育する: 破損パッケージ攻撃に関連するリスクと、インストール前にパッケージを検証することの重要性を開発者に教育します。



**参考資料:**
