---
layout: col-sidebar
type: documentation
altfooter: true
level: 4
auto-migrated: 0
document: OWASP Machine Learning Security Top Ten 2023
year: 2023
order: 1
title: ML01:2023 Adversarial Attack (敵対的攻撃)
lang: en
tags:
  [
    OWASP Machine Learning Security Top Ten 2023,
    Top Ten,
    ML01:2023,
    mltop10,
    mlsectop10,
  ]
exploitability: 5
detectability: 3
technical: 5
---

## 説明

敵対的攻撃は攻撃者が意図的に入力データを改竄してモデルを欺く攻撃の一種です。


## 防止方法

**敵対的トレーニング:** 敵対的攻撃から防御するアプローチの一つは敵対的事例でモデルを訓練することです。
これによりモデルは攻撃に対してより堅牢になり、欺かれにくくなります。


**ロバストモデル:** もう一つのアプローチは敵対的トレーニングや防御メカニズムを組み込んだモデルなど、敵対的攻撃に対して堅牢になるように設計されたモデルを使用することです。



**入力バリデーション:** 入力バリデーションは敵対的攻撃の検知と防止に使用できるもう一つの重要な防御メカニズムです。
これは予期しない値やパターンなど異常がないか入力データをチェックし、悪意のある可能性が高い入力を拒否するものです。



## リスク要素

| 脅威エージェント/攻撃手法 | セキュリティ上の弱点 | 影響 |
| ------------------------- | :------------------: | :--: |
| 悪用難易度: 5 (容易) <br><br> _ML アプリケーション依存: 4_ <br> _ML オペレーション依存: 3_ | 検出難易度: 3 (普通) <br><br> _敵対的画像は肉眼では識別できず、攻撃の検出は困難になることがあります。_ | 技術的影響: 5 (深刻) <br><br> _攻撃にはディープラーニングや画像処理技術の専門知識が必要です。_ |
| 脅威エージェント: ディープラーニングや画像処理技術の知識を持つ攻撃者。 <br><br> 攻撃手法: 正規の画像に類似して意図的に作られた敵対的画像。 | 画像を正確に分類するディープラーニングモデルの能力における脆弱性。 | 画像の誤分類により、セキュリティのバイパスやシステムへの侵害につながります。 |

本チャートは [下記のシナリオ](#scenario1) に基づくサンプルに過ぎないことに注意することが重要です。
実際のリスク評価は各機械学習システムの具体的な状況によって異なります。


## 攻撃シナリオの例

### シナリオ \#1: 画像分類 {#scenario1}

ディープラーニングモデルは画像を犬や猫などのさまざまなカテゴリに分類するように訓練されています。
攻撃者は正規の猫の画像に非常によく似ていますが、慎重に作成された小さな摂動により、モデルがそれを犬として誤分類するような敵対的画像を作成します。
モデルが実際の環境にデプロイされると、攻撃者は敵対的画像を使用してセキュリティをバイパスしたりシステムに侵害したりできます。




### シナリオ \#2: ネットワーク侵入検知

ディープラーニングモデルはネットワークへの侵入を検知するように訓練されています。
攻撃者はモデルの侵入検知システムを回避するように慎重にパケットを作成することにより、敵対的ネットワークトラフィックを作り出します。
攻撃者は送信元 IP アドレス、宛先 IP アドレス、ペイロードなどのネットワークトラフィックの特性を侵入検知システムに検知されないように操作できます。
たとえば、攻撃者が送信元 IP アドレスをプロキシサーバーの背後に隠したり、ネットワークトラフィックのペイロードを暗号化する可能性があります。
この種の攻撃はデータ窃取、システム侵害、その他の形態の損害につながる可能性があるため、深刻な結果をもたらす可能性があります。





## 参考資料
